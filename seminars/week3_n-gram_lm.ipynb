{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "k1gpzj4guo8e1riwj3om1k"
   },
   "source": [
    "### Семинар 3. Языковые модели (N-gram)\n",
    "\n",
    "В этом семинаре мы построим простейшую языковую модель генерации анекдотов. Датасет взят [отсюда](https://t.me/NeuralShit/2321)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "cellId": "u8jdaiy68oib3jvr4k01"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "cellId": "0c76vnyl3zui9yhtkodgrlf"
   },
   "outputs": [],
   "source": [
    "with open('anek.txt', 'r') as f:\n",
    "    aneki = f.read().strip().replace('<|startoftext|>', '').split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кто сказал, что солдат мечтает стать генералом? Солдат мечтает стать хлеборезом.'"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aneki[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7u97m5s8ekl5zd5a43a1yc"
   },
   "source": [
    "### Токениация\n",
    "\n",
    "Реализуем два варианта токенизации: обычную по словам и BPE. В дальнейшем будем их сравнивать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct_bpe - proportion of tokens that obtained via BPE. Other are the most frequent words.\n",
    "encoder = Encoder(50000, ngram_max=6, pct_bpe=0.95)\n",
    "encoder.fit(aneki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47500"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.bpe_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('__eow', 2500),\n",
       "  ('__sow', 2501),\n",
       "  ('о', 2502),\n",
       "  ('а', 2503),\n",
       "  ('е', 2504),\n",
       "  ('и', 2505),\n",
       "  ('т', 2506),\n",
       "  ('н', 2507),\n",
       "  ('р', 2508),\n",
       "  ('с', 2509)],\n",
       " [('ньюто', 49990),\n",
       "  ('ньютон', 49991),\n",
       "  ('геи', 49992),\n",
       "  ('дилы', 49993),\n",
       "  ('одилы', 49994),\n",
       "  ('абар', 49995),\n",
       "  ('заха', 49996),\n",
       "  ('еревни', 49997),\n",
       "  ('енятьс', 49998),\n",
       "  ('изюм', 49999)])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.bpe_vocab.items())[:10], list(encoder.bpe_vocab.items())[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['кто', 'сказал', ',', 'что', 'солдат', '__sow', 'мечтае', 'т', '__eow', 'стать', '__sow', 'генера', 'лом', '__eow', '?', 'солдат', '__sow', 'мечтае', 'т', '__eow', 'стать', '__sow', 'хлебо', 'резо', 'м', '__eow', '.']\n",
      "[59, 172, 2, 9, 1509, 2501, 20745, 2506, 2500, 385, 2501, 15260, 3297, 2500, 23, 1509, 2501, 20745, 2506, 2500, 385, 2501, 25012, 26588, 2441, 2500, 3]\n",
      "кто сказал , что солдат мечтает стать генералом ? солдат мечтает стать хлеборезом .\n"
     ]
    }
   ],
   "source": [
    "example = aneki[42]\n",
    "print(encoder.tokenize(example))\n",
    "print(next(encoder.transform([example])))\n",
    "print(next(encoder.inverse_transform(encoder.transform([example]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bpe(text):\n",
    "    tokenized = encoder.tokenize(text)\n",
    "    clear_tokenized = []\n",
    "    first = False\n",
    "    saw_eow = True\n",
    "    for token in tokenized:\n",
    "        if token == '__sow':\n",
    "            saw_eow = False\n",
    "            first = True\n",
    "            continue\n",
    "        elif token == '__eow':\n",
    "            saw_eow = True\n",
    "            continue\n",
    "        else:\n",
    "            if first or saw_eow:\n",
    "                clear_tokenized.append(token)\n",
    "                first = False\n",
    "            else:\n",
    "                clear_tokenized.append('##' + token)\n",
    "    return clear_tokenized\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    reg = re.compile(r'\\w+')\n",
    "    return reg.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['кто',\n",
       " 'сказал',\n",
       " ',',\n",
       " 'что',\n",
       " 'солдат',\n",
       " 'мечтае',\n",
       " '##т',\n",
       " 'стать',\n",
       " 'генера',\n",
       " '##лом',\n",
       " '?',\n",
       " 'солдат',\n",
       " 'мечтае',\n",
       " '##т',\n",
       " 'стать',\n",
       " 'хлебо',\n",
       " '##резо',\n",
       " '##м',\n",
       " '.']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_bpe(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qb6h3hxmr095egzv8rlzul"
   },
   "source": [
    "### N-граммная языковая модель\n",
    "\n",
    "Языковая модель – это вероятностная модель, которая считает вероятность последовательности токенов $P(w_1, \\dots, w_T)$. Так как оценивать совместную вероятность в лоб тяжело, обычно ее разбивают на произведение условных вероятностей. \n",
    "\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = P(w_1)\\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_1)\n",
    "$$ \n",
    "\n",
    "На практике такие условные вероятности сложно оценивать, когда текст очень длинный. Языковые модели лучше всего работают с небольшим контекстом. Для решения этой проблемы можно явно ограничить длину контекста, записав такое предположение\n",
    "$$\n",
    "P(w_i \\mid w_{i-1}, \\dots, w_1) \\approx P(w_i \\mid w_{i-1}, \\dots, w_{i-n+1}).\n",
    "$$\n",
    "\n",
    "Данная модель называется __n-граммной языковой моделью__, так как оценивает вероятности только n-грамм токенов. Тогда итоговая вероятность последовательности токенов записывается вот так\n",
    "\n",
    "$$\n",
    "P(w_1, \\dots, w_T) = \\prod_{i=1}^T P(w_i \\mid w_{i-1}, \\dots, w_{i-n+1}).\n",
    "$$\n",
    "\n",
    "Для начало последовательности можно добавить специальные токены `[UNK]`, чтобы в условии всегда был контекст фиксированной длины.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u68wydbiioqlp5gl96mhd"
   },
   "source": [
    "В этом семинаре мы не будем ничего учить, наша модель будет счетной. Поэтому для начала нам надо посчитать, сколько раз встречается каждая n-грамма. В начало последовательности будем добавлять  `[UNK]`, а в конец – `[EOS]`. При генерации модель будет выдавать `[EOS]`, когда настанет время остановиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "cellId": "og84gjipnumsakhiiu9ap"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "UNK, EOS = \"[UNK]\", \"[EOS]\"\n",
    "\n",
    "def count_ngrams(lines, n, tokenize=tokenize):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    Input: a list of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    If the prefix is too short, it should be padded with [UNK].\n",
    "    Add [EOS] at the end of each sequence and consider it as all other token\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "\n",
    "    for line in lines:\n",
    "        tokenized = [UNK] * (n - 1) + tokenize(line) + [EOS]\n",
    "        for i in range(n - 1, len(tokenized)):\n",
    "            counts[tuple(tokenized[i-n+1:i])][tokenized[i]] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Последним раскрытым громким преступлением в Киеве было убийство Столыпина...',\n",
       " 'Если бесконечное количество российских футболистов запустить на бесконечное количество футбольных полей и дать им бесконечное количество времени, то один из них когда-нибудь забьёт гол.',\n",
       " 'На чемпионат мира по футболу от России нужно Юлию Самойлову отправлять, хоть какая-то надежда на победу будет.',\n",
       " 'В целях профилактики от всего весной следует есть много чеснока. От женщин, кстати, тоже помогает.',\n",
       " 'На моих глазах как-то две девушки затаскивали кавказца в машину. Они худенькие, а он здоровый такой, никак не хотел в машину лезть. Они попросили у меня помощи, сказали, что собаку надо в ветклинику отвезти.']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_lines = aneki[-5:]\n",
    "dummy_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "cellId": "xyf2he6lak9mmqarl3nck"
   },
   "outputs": [],
   "source": [
    "dummy_counts = count_ngrams(dummy_lines, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'в': 1})"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_counts[('громким', 'преступлением')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'последним': 1, 'если': 1, 'на': 2, 'в': 1})"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_counts[(UNK, UNK)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4j620npeqvj0k8ak8xqx8xk"
   },
   "source": [
    "Теперь мы можем оценить вероятности, используя посчитанные n-граммы.\n",
    "\n",
    "$$ P(w_i | prefix) = \\frac{Count(prefix, w_i)}{\\sum_{w \\in V} Count(prefix, w)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n=3, tokenize=tokenize):\n",
    "\n",
    "        counts = count_ngrams(corpus, n, tokenize=tokenize)\n",
    "        self.n = n\n",
    "\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        # calculate the probabilities using the formula above\n",
    "        for prefix, token_count in counts.items():\n",
    "            token_sum = sum(token_count.values())\n",
    "            for token, count in token_count.items():\n",
    "                self.probs[prefix][token] = count / token_sum\n",
    "        \n",
    "    def process_prefix(self, prefix):\n",
    "        if self.n == 1:\n",
    "            prefix = []\n",
    "        else:\n",
    "            prefix = prefix[-(self.n - 1):]\n",
    "            prefix = [UNK] * (self.n - 1 - len(prefix)) + prefix\n",
    "            \n",
    "        return prefix\n",
    "\n",
    "    def get_tokens_and_probs(self, prefix):\n",
    "        prefix = self.process_prefix(prefix)\n",
    "\n",
    "        possible_tokens = self.probs[tuple(prefix)]\n",
    "\n",
    "        tokens = list(possible_tokens.keys())\n",
    "        probs = list(possible_tokens.values())\n",
    "\n",
    "        return tokens, probs\n",
    "    \n",
    "    def get_token_prob(self, token, prefix):\n",
    "        prefix = self.process_prefix(prefix)\n",
    "\n",
    "        prob = self.probs[tuple(prefix)].get(token, 0)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oh8r9a41kuk4r51wra9"
   },
   "source": [
    "Наконец, мы можем использовать полученную модель для генерации анекдотов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "cellId": "f17xoejjppmooo2nopw4xo"
   },
   "outputs": [],
   "source": [
    "lm = NGramLanguageModel(aneki, n=3, tokenize=tokenize_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2kd9glwnkr470qc4bt7f1e"
   },
   "source": [
    "Процесс генерации всегда авторегрессионный. Это значит, что выход модели на предыдущем шаге поступает на вход следующего. Таким образом можно бесконечно генерировать текст (ну или до тех пор, пока модель не выдаст [EOS]).\n",
    "\n",
    "Для выбора одного токена из всех возможных вариантов существует огромное количество техник. Например, можно брать самый вероятный или семплировать токен в соответствии с вероятностями. Более подробно обсудим это на 4 семинаре. Мы остановимся на втором подходе, чтобы каждый раз получались разные тексты.\n",
    "\n",
    "$$w_{next} \\sim \\frac{P(w_{next} | prefix)}{\\sum_{w} P(w | prefix)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "cellId": "sgbatlm9vzb4z889fho7"
   },
   "outputs": [],
   "source": [
    "def get_next_token(lm, prefix):\n",
    "    tokens, probs = lm.get_tokens_and_probs(prefix)\n",
    "\n",
    "    next_token = np.random.choice(tokens, p=probs)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "cellId": "1nnnycga61rijt6nd8zai"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "мужчина стоит на ногах . [EOS]\n"
     ]
    }
   ],
   "source": [
    "prefix = tokenize('мужчина')\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += [get_next_token(lm, prefix)]\n",
    "    if prefix[-1] == EOS or len(lm.get_tokens_and_probs(prefix)[0]) == 0:\n",
    "        break\n",
    "\n",
    "print(' '.join(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3gdmey7g8at5n5c5x4gayh"
   },
   "source": [
    "### Оценка качества языковой модели: перплексия\n",
    "\n",
    "Перплексия оценивает то, насколько хорошо модель предсказывает распределение данных. Она считатся по данной формуле:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_T) = PPL(w_1, \\dots, w_T)^{-\\frac{1}{T}} = \\left( \\prod_i P(w_i \\mid w_{i-1}, \\dots, w_{i - n + 1})\\right)^{-\\frac{1}{T}},\n",
    "$$\n",
    "\n",
    "Можно заметить, что это в точности экспонента кросс-энтропии. Поэтому, чем меньше перплексия, тем лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "cellId": "5hp010xyzzb4vqewo1bhny"
   },
   "outputs": [],
   "source": [
    "def perplexity(lm, lines, min_prob=10 ** -50., tokenize=tokenize):\n",
    "    \"\"\"\n",
    "    :param min_prob: if P(w | ...) is smaller than min_prop, set it to min_prob.\n",
    "    :returns: mean perplexity over the whole corpus\n",
    "    \"\"\"\n",
    "\n",
    "    ppls = []\n",
    "    for line in tqdm(lines):\n",
    "        tokenized = tokenize(line)\n",
    "        log_ppl = 0\n",
    "        for i in range(len(tokenized)):\n",
    "            log_ppl += np.log(max(\n",
    "                min_prob,\n",
    "                lm.get_token_prob(tokenized[i], tokenized[:i])\n",
    "            ))\n",
    "        ppls.append(np.exp(-log_ppl / len(tokenized)))\n",
    "\n",
    "    return np.mean(ppls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f160ad4011a3478ea2477fbf193c385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf383c03eb24f2dbdf167653f56d386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64206f8d9234763aae96efe299b6eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=81.663 ppx3=1.142 ppx10=1.103\n"
     ]
    }
   ],
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ypc4lks4vs1li908fqi8"
   },
   "source": [
    "Теперь мы можем посчитать перплексию нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "cellId": "tjnehsem2lmijkg2lto4w"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88a34779f9143c9b0049188e988b0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 12887014247997405467423213088777838380250562560.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd4c3aa9fb042f197949cdd2a677184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 460881476685444201036843739570228114189619036160.00000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc6a31ba6ef45e38b7841ff6371a6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 3010794325323826519754827834236485472713017655296.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(aneki, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(train_lines, n=n)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oopn2o57wxm9vbxzycytce"
   },
   "source": [
    "### Сглаживание вероятностей\n",
    "\n",
    "Проблема заключается в следующем. Каждый раз, когда модель встречает в тестовом корпусе n-грамму, которой не было в тренировочном, она присваивает ей нулевую вероятность. Соответствено, все произведение зануляется, независимо от того, как выглядит остальной текст.\n",
    "\n",
    "Один из способов обойти это – добавить сглаживание вероятностей ([сглаживание Лапласа](https://en.wikipedia.org/wiki/Additive_smoothing)). Сделаем вид, что мы видели каждую n-грамму хотя бы один раз. При наличии достаточно большого корпуса, это почти не поменяет распределения вероятностей, но зато позволит нам не взрывать перплексию.\n",
    "\n",
    "$$ P(w_t \\mid prefix) = \\frac{Count(prefix, w_t) + \\delta}{\\sum_{w \\in V} \\big(Count(prefix, w) + \\delta\\big)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "cellId": "ioh26rlov6g8l2ssj1c8pm"
   },
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel):\n",
    "    def __init__(self, corpus, n, delta=1.0, tokenize=tokenize):\n",
    "\n",
    "        counts = count_ngrams(corpus, n, tokenize=tokenize)\n",
    "        self.n = n\n",
    "        self.vocab = set()\n",
    "        for token_count in counts.values():\n",
    "            self.vocab |= set(token_count.keys())\n",
    "\n",
    "        self.probs = defaultdict(Counter)\n",
    "        for prefix, token_count in counts.items():\n",
    "            total = sum(token_count.values()) + delta * len(self.vocab)\n",
    "            for token, count in token_count.items():\n",
    "                self.probs[prefix][token] = (count + delta) / total\n",
    "\n",
    "    def get_tokens_and_probs(self, prefix):\n",
    "        # we want to spread some propability among all tokens\n",
    "        \n",
    "        tokens, probs = super().get_possible_next_tokens(prefix)\n",
    "        \n",
    "        left_prob = 1.0 - sum(probs)\n",
    "        unseen_prob = left_prob / max(1, len(self.vocab) - len(tokens))\n",
    "        \n",
    "        unseen_tokens = self.vocab - set(tokens)\n",
    "\n",
    "        return tokens + list(unseen_tokens), probs + [unseen_prob] * len(unseen_tokens)\n",
    "\n",
    "    def get_token_prob(self, token, prefix):\n",
    "        prob = super().get_token_prob(token, prefix)\n",
    "        if prob > 0:\n",
    "            return prob\n",
    "\n",
    "        tokens, probs = super().get_tokens_and_probs(prefix)\n",
    "\n",
    "        left_prob = max(1e-8, 1.0 - sum(probs))\n",
    "        unseen_prob = left_prob / max(1, len(self.vocab) - len(tokens))\n",
    "\n",
    "        return unseen_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "cellId": "3xvxkdxcmfqucruyt66mdc"
   },
   "outputs": [],
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert sum(([dummy_lm.get_token_prob(w_i, ['l']) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613bf66eec0647ccb03187fc41ccbce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 39955.87579\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3370820c995496a92e6f9ad8d41eba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 30058.30273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970d0503859a4954a59ae3f4226ff907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 70786.18871\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c14a9bb7346436f96f16d61c2c9d973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 2695.94599\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038200b64902414a85fd76c0b10d86a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2, Perplexity = 3860.07277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c45d5f780ee4785874744beae5989ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31039 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 3, Perplexity = 15598.23201\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=1, tokenize=tokenize_bpe)\n",
    "    ppx = perplexity(lm, test_lines, tokenize=tokenize_bpe)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
