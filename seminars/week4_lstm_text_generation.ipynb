{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e949660-4e0b-4f7b-a074-293e2c91cc60",
   "metadata": {
    "cellId": "k1gpzj4guo8e1riwj3om1k"
   },
   "source": [
    "### Семинар 4. Генерация текста: рекуррентные нейронные сети\n",
    "\n",
    "В этом семинаре мы продолжим учиться генерировать анекдоты. На этот раз с помощью LSTM. Датасет взят [отсюда](https://t.me/NeuralShit/2321)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d424ffdf-0f73-41e0-b13b-a8d1008027f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd5b45-e8ec-4ed2-86eb-119dba67b239",
   "metadata": {
    "cellId": "u8jdaiy68oib3jvr4k01"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc413d8-ef12-4e18-8536-be1f2fc9baed",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf5be4b-cad8-414f-804d-e8462c356ebe",
   "metadata": {
    "cellId": "0c76vnyl3zui9yhtkodgrlf"
   },
   "outputs": [],
   "source": [
    "with open('anek.txt', 'r') as f:\n",
    "    aneki = f.read().strip().replace('<|startoftext|>', '').split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5447b486-0164-48a3-ac38-f9b8d7717bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кто сказал, что солдат мечтает стать генералом? Солдат мечтает стать хлеборезом.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aneki[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bb55e-c462-47eb-96db-3f836bb30879",
   "metadata": {},
   "source": [
    "Поделим выборку на тренировочную и тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c73f27f-1c85-4560-b0b9-f98294f026a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2fc087-d6c0-436b-b8e7-16d8da278868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts = train_test_split(aneki, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf59f70-2a56-4860-b014-4135af581625",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_texts, batch_size=128, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(train_texts, batch_size=128, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d95fe2-8438-45fd-a849-011f8bb240a6",
   "metadata": {
    "cellId": "7u97m5s8ekl5zd5a43a1yc",
    "tags": []
   },
   "source": [
    "### Токениация\n",
    "\n",
    "В качестве токенизатора возьмем предобученный из библиотеки [hugging face](https://huggingface.co). Он имеет относительно небольшой размер словаря, что хорошо для нашей легковестной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9656f9f6-ff19-416d-9d4b-29e504cc54b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29564"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555f3063-c985-49d5-9b41-9338bd82072f",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "LSTM – это рекуррентная нейронная сеть, обладающая модулем памяти, который позволяет есть улавливать долгосрочные зависимости. Ее скрытое состояние в вектор памяти обновляются на каждом шаге по следующим формулам:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "f_t=\\sigma\\left(W_f \\cdot\\left[h_{t-1}, x_t\\right]+b_f\\right) \\\\\n",
    "i_t=\\sigma\\left(W_i \\cdot\\left[h_{t-1}, x_t\\right]+b_i\\right) \\\\\n",
    "o_t=\\sigma\\left(W_o \\cdot\\left[h_{t-1}, x_t\\right]+b_o\\right) \\\\\n",
    "\\tilde{C}_t=\\tanh \\left(W_c \\cdot\\left[h_{t-1}, x_t\\right]+b_c\\right) \\\\\n",
    "C_t=f_t \\odot C_{t-1}+i_t \\odot \\tilde{C}_t \\\\\n",
    "h_t=o_t \\odot \\tanh \\left(C_t\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Мы не будем учить модель, потому что на это уйдет весь семинар, поэтому возьмем заранее обученную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c029bb8a-a394-427a-b39f-913dd2321563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LSTM\n",
    "from train import train\n",
    "from train import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f1b57f5-e9cb-477b-9d42-cc233977d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm = LSTM(len(tokenizer), input_size=256, hidden_size=256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48255bb8-ee48-4a57-8c37-f725dbe8d04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.load_state_dict(torch.load('aneki_lstm_256.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aec0b1d5-fac2-4be0-b074-b65db60b8370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e825f67c034bb5b6b9d2b6af233006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3197522737589352"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train import evaluate\n",
    "\n",
    "test_accuracy = evaluate(lstm, test_loader, tokenizer)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b04605a4-4ebc-4c2b-b11b-78bc554ff2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amshabalin/anaconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(32.7403, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import perplexity\n",
    "\n",
    "ppl = perplexity(lstm, train_texts[:100], tokenizer)\n",
    "ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61c3ee-58f4-4ba2-bde6-f99f110fbdd3",
   "metadata": {},
   "source": [
    "## Оценка качества\n",
    "\n",
    "В этой секции мы будем тестировать различные методы семплирования токенов. Для того, чтобы их можно было как-то сравнивать, необходимо ввести метрики качества. Мы будем отслеживать кросс-энтропийную ошибку большой и мощной языковой модели ([ruGPT](https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2), если быть точнее), долю уникальных слов и энтропию предсказаний нашей модели.\n",
    "\n",
    "Для начала посчитаем значения метрик для оригинального текста. Это нужно, чтобы мы знали, к каким цифрам стремиться. Оценивать качество будем по 5000 текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f31c060-04e3-4770-a17b-44a33e4e654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_texts = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "266b5577-5a39-472e-81f8-1911db42292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc68684f-7c99-4d1c-950f-5191c0066cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "gpt_loss = GPTLoss(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2613b3-850a-45dc-aefc-e2af946d0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "for texts in train_loader:\n",
    "    train_texts.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0655bd6d-941c-4509-9fb6-e0b154609c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Дима, давай быстрее собирайся, скоро начинаем открытие Олимпиады!- Вова? Я спать хочу!- Задрал, там поспишь!',\n",
       " 'Работник мясокомбината пытался вынести со склада продукцию, и был пойман с потрохами.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17ace67f-59e6-4da4-bf16-ae7601ea78dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2522258276845618"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(train_texts[:n_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05cadfac-370a-465c-883f-ee151f10439f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade61803720642aa8ece6983d21d940e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.5969808605536358"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_gpt_loss = gpt_loss.compute(train_texts[:n_texts])\n",
    "source_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6faec6-05d9-4d1e-886a-cdecee88520b",
   "metadata": {},
   "source": [
    "Реализуем функцию для генерации текста. Одно из приятных свойств рекуррентных нейронных сетей заключается в том, что для предсказания следующего токена нам не нужно пробегать всю последовательность заново. Мы можем просто подать модели на вход последний сгенерированный токен и текущее скрытое состояние."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7024d3fb-61fd-4f77-aaaa-5190b767245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, sampler, n_samples=1):\n",
    "    model.eval()\n",
    "\n",
    "    cur_tokens = torch.full((n_samples, 1), tokenizer.cls_token_id, device=device)\n",
    "    output_ids = cur_tokens.clone()\n",
    "\n",
    "    h_t = torch.zeros(n_samples, 1, model.hidden_size, device=device)\n",
    "    c_t = torch.zeros(n_samples, 1, model.hidden_size, device=device)\n",
    "\n",
    "    all_entropies = []\n",
    "    ended_texts = torch.zeros(n_samples)\n",
    "    for i in range(256):\n",
    "        output, (h_t, c_t) = model(cur_tokens, init_states=(h_t.squeeze(1), c_t.squeeze(1)))\n",
    "\n",
    "        next_tokens, token_entropy = sampler.sample(output[:, -1].cpu())\n",
    "        all_entropies.extend(token_entropy[ended_texts == 0])\n",
    "\n",
    "        cur_tokens = next_tokens.unsqueeze(1).to(device)\n",
    "        output_ids = torch.cat((output_ids, cur_tokens), dim=1)\n",
    "        \n",
    "        ended_texts += (next_tokens.cpu() == tokenizer.sep_token_id)\n",
    "        if torch.all(ended_texts > 0):\n",
    "            break\n",
    "\n",
    "    return tokenizer.batch_decode(output_ids), np.mean(all_entropies)\n",
    "\n",
    "def generate_n_texts(model, sampler, n_texts=1000):\n",
    "    batch_size = 250\n",
    "    generated_texts = []\n",
    "    entropies = []\n",
    "    for _ in tqdm(range(0, n_texts // batch_size)):\n",
    "        texts, entropy = generate(model, sampler, n_samples=batch_size)\n",
    "        generated_texts.extend(clear_text(texts))\n",
    "        entropies.append(entropy)\n",
    "        \n",
    "    return generated_texts, np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6daa979-64df-4dbd-b5c4-a2efb07fceb0",
   "metadata": {},
   "source": [
    "В классе `Sampler` реализованны разничные способы семлирования токенов. Код этого класса посмотреть нельзя, так как задание на реализацию этих способов будет в домашке.\n",
    "\n",
    "Начнем с жадного метода сэмплирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80919019-7a66-4122-a133-a2c39079352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampler import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce158c81-049c-4400-87e9-6106e7fdb075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960123dacd3046c99793fb753b2766bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = Sampler(prob_mode='temperature', sample_mode='greedy', temp=1)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bfd9445-8ea7-44d5-85f4-8908cf16e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- А ты знаешь, что у вас есть? - Да, а что? - А ты что, не знаешь, что это такое? - Да, а что? - А я не могу...',\n",
       " '- А ты знаешь, что у вас есть? - Да, а что? - А ты что, не знаешь, что это такое? - Да, а что? - А я не могу...',\n",
       " '- А ты знаешь, что у вас есть? - Да, а что? - А ты что, не знаешь, что это такое? - Да, а что? - А я не могу...']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12351fe-65d4-4696-a0fb-a2882fe19f03",
   "metadata": {},
   "source": [
    "Так как никакого элемента случайности в генерации нет, тексты получились одинаковыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e89b3fa-b72a-4921-9b9d-cbe66af7b08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.000104, 0.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53851121-7217-47a3-93f4-0e2c3ed42fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7679a3db3d54bdebb995c252cf13f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2.181544780731201"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37eae86-828d-45b7-9a60-4ec61451d6d9",
   "metadata": {},
   "source": [
    "Конечно же, много уникальных токенов в 5000 одинаковых текстах не найдется. Энтропию тоже нет смысла считать. Ошибка большой языковой модели оказалась ниже, чем у оригинальных текстов, потому что сгенерированные тексты содержат повторения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00ea3cb1-be69-456f-ba2c-faaff60672dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d997382e767c44a088b2dbe3bbfa9724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.41997745633125305"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repetitions_gpt_loss = gpt_loss.compute(['Я не буду тратить мел. ' * 20])\n",
    "repetitions_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a6948-0196-4a6b-a7df-d456ae8c0595",
   "metadata": {},
   "source": [
    "#### Семплирование с температурой\n",
    "\n",
    "Попробуем теперь семплировать токены случайно в соответствии с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea2dd66e-dd94-4364-9be1-0630e80bed77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111d71ac2854d75b2393b7b6b450e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = Sampler(prob_mode='temperature', sample_mode='random', temp=1)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "41ed8ab4-41f9-43dc-ada8-4ac68c43617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Скажите, а почему пригласили вечеринвы за важной перед страхов Гаинец, а во мне встретили начинается как в будущем? - Возьмите жену в носа.',\n",
       " '- Девушка, вы откуда привыкрасным? - А куда вам закусить соседиб от тебя! - Я услышал, как в аду будут, чтобы на язык подожди радостноцизине советует, что вы кошек не знает чем с простого приснымся.',\n",
       " 'Утро женщина - это когда с собой не появляется.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d89b499-abc5-4da2-9468-4b6d98a0e6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.281033790117395, 3.6328158)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4aeb8445-84e2-4e20-8828-306c2c9e3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003f19f12419498295f69c585c2b7710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.998079306005547"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fef6d-8827-4c02-b50a-6228fa7ee35b",
   "metadata": {},
   "source": [
    "Если взять температуру поменьше, то распределение вероятностей станет более вырожденным и, как следствие, энтропия уменьшится. При этом уменьшится и доля уникальных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d7ca2e6-647b-4b2d-9be2-de9d093118ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa5faa9022b466b9f2f9e70c05d677b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = Sampler(prob_mode='temperature', sample_mode='random', temp=0.7)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3abe3186-e856-4d1a-9b32-7289561a2417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Дура, ты чего такой длиннее, чем вас вернуть? - А в чем разница у тебя? - Я не знаю, что я тебе внутреннюю аптекарный.',\n",
       " 'Бесит, когда в подъезде нельзя надо покупать.',\n",
       " '- Какие красивое, я - ни разу так красивыми? - А что это? - А что ты был? - Да, я ношу и открывает бухгалтера.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d5d8c04-8f30-4621-b7ed-3687c5f825d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13245934610384832, 2.3059564)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "acbbc137-748d-4472-abd6-7e7a23c2caf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbb7b2a8ca649be8080f4f950452b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.407580097166828"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7357b3-fcc1-4a5d-b125-2903cc943303",
   "metadata": {},
   "source": [
    "Соответственно, если увеличить температуру, распределение вероятностей станет более равномерным и энтропия увеличится. Так же, редкие слова начнут семплироваться чаще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d58d05b2-c0b8-470d-90c2-405f22fbc5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1580190658548aaad0dbd085cfa5c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = Sampler(prob_mode='temperature', sample_mode='random', temp=1.5)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6dddc92f-2822-492f-a1c3-69a734d6176e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Новый год начали выпускают женщин о Трой часы для к ней в маршрутке, вылавиши себе новую сумку цески берет документы этой, дня назад два версии Калипрасно вышкосоксисляют чем Николай.',\n",
       " 'Сеев змея как Украине звый бралася перед Потапдому музыку починит к любой бум гаечности по Зеленоре, Мэнбек нет я быстрокотсия с тобой говорить им один самогоусенников.',\n",
       " 'Це несла такие та! Это уже перек мастериажат соглашение стройные тяжеляли давно в карманах Майкловые черном...., этим хотел правители. Hе женщины им член Ивановна.ил. Остантирушка игр в предкле искусств можно.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "236d214e-7a59-496a-992b-b802fc9e1c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35940269370071326, 5.8276668)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c15a8a3d-8cf7-4e62-87e8-9263491bdc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb568cb1b7d64f9db658298768da92c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.564157451305716"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8830ceb8-38a0-4d6e-b9dc-e068ceed3e10",
   "metadata": {},
   "source": [
    "#### topk семплирование\n",
    "\n",
    "Тут применяется схожая логика со случаем семплирования с температурой. Выбирая небольшое k, мы начинаем семплировать только самые вероятные токены, и такое поведение отражается на метриках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "205bf949-fd8a-4fb1-8a56-b268108a89fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da8c8b7b2bf4d0ab9caaffd8d0588bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = Sampler(prob_mode='topk', sample_mode='random', k=50)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc54daa5-22f6-4915-9907-6642ba5709b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Скажите, вы уже сзади! - Это чео! Я не хочу посоветовать.',\n",
       " 'Девушка, вы так себе думаешь, что в этом году в России стали меньше вечной сексу с медведя. Может, это не знал, что вы поняли, купи мне колгот?',\n",
       " '- Вовочка, ты чего собираешься? - Да, внучек! Я считаю!']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "78f96f6d-0d27-4e69-87b2-e06bb5cac8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18469160722068031, 2.6243415)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c5121de7-f562-4ffc-9876-2c06a870e250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5520a51621549cd98521dbb30774ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.250501114559838"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75da2c2-c741-41e2-abcf-3a574e94521b",
   "metadata": {},
   "source": [
    "Увеличивая k, мы понижаем вероятность каждого отдельного токена, и это тоже видно на метриках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "058159ca-7778-4586-8e96-7d1ff0780a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27cd46b98614cebbb52cd849b913654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = Sampler(prob_mode='topk', sample_mode='random', k=500)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=n_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7eb7d6b6-e61c-47e7-b98c-1ab9282e28f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В Одессе засматривая Чака молодецмана России, это прекращается по ночам : - Может ли вы зеркало - то, что смогу жить сейчас на ужином? - Нет, я а это лучше, чем вы ума, выбор еще и дают.',\n",
       " 'Запилишь работу так, как будто выучил опытный компьютерщика или поменько, а потом вызвали у девяного испанию.',\n",
       " 'В магазине : - Сидоров, ты откуда и так получилось за нас замечали, то мне было 30 лет? - Плохой старуг в первый раз - стоишь трубку.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d58269c-749a-43a0-b35c-89c769ce427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26616300060172493, 3.45268)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7df52ea3-40dc-4261-8c1e-3c051e855900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4127616ccf9949a5add536d27394f26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.9103927530825775"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90cddd-4fa8-4a69-a1dc-8a8db257418b",
   "metadata": {},
   "source": [
    "#### Nucleus sampling\n",
    "\n",
    "Значение 0.95 для параметра p считается самым удачным, для нашей модели оно тоже отлично подходит с точки зрения доли уникальных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ba3d5-8616-4613-82f2-9af8c74c75fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(prob_mode='temperature', sample_mode='nucleus', temp=1, p=0.95)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "61107916-5e62-4d7b-8bae-39dd58f12eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Многие ученые появится парочка жениху, когда он пропилишься без трусики куплю - с собой лотереться, давай влюбленная плавание нации.',\n",
       " 'Мой друг спрашивают : - Что твоя мама в парад - финансы, которая водка? - Только когда я собака... Семнадцать вольманная взгляда, не интересуется, ты ему ответила в целом замельтеку...',\n",
       " 'Шашен национальности, вы не делай олимпийским миллионеры : им предупреди счастливой рабочие дни в эфир.']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d81cfd2d-cccb-48d4-aad5-d7891e83e12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39821234171775627, 0.38607075913776945, 3.2875628)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(train_texts[:1000]), unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fabec02c-5a05-4e13-b841-793ab82a0c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02060c2e892d491f997175f791047ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.92559085117137"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b733d-1024-42fe-bbc6-4ef46ec7bf3d",
   "metadata": {},
   "source": [
    "Уменьшение p приведет нас к менее разнообразному тексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af818911-1be4-447b-9e6f-1c4fa7c58395",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = Sampler(prob_mode='temperature', sample_mode='nucleus', temp=1, p=0.7)\n",
    "\n",
    "generated_texts, entropy = generate_n_texts(lstm, sampler, n_texts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7ff03bd6-1d5a-4417-81fa-9660db7195c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Колдунья в примету : - У тебя жена звонит другу, так не кусает! - Молодой человек, вы круточку на 8 марта?',\n",
       " '- Ты куда тебя поймал? - Ты понимаешь, какой ты мне подумать?! - Да, возьмите...',\n",
       " 'Объявление. Вова, когда он приходит домой, отбирается с родителями и собирали им дочке : - И как же сейчас придется?! - Да...']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "029d58df-5e10-4f0d-918d-cbfe17197b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2524177949709865, 2.2645855)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words_rate(generated_texts), entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "085b2542-7134-4296-b0cf-b2d9db9f83b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f605a684af40b2bc59708b8efdae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.8342101303367"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gpt_loss = gpt_loss.compute(generated_texts)\n",
    "lstm_gpt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09f7fc0-eb23-4f16-8241-f901ef886236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
