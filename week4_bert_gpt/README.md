# BERT и GPT

На этой неделе мы обсуждаем две главные модели на основе Трансформера – BERT и GPT. Для каждой из них мы рассматриваем сценарии использования и обсуждаем, почему они оказались настолько успешны. В конце мы говорим о трюках для обучения трансформерных моделей.

## Статьи по теме

1. BERT: [статья](https://arxiv.org/abs/1810.04805), [блогпост с иллюстрациями](https://jalammar.github.io/illustrated-bert/).
1. GPT: [статья](https://arxiv.org/abs/2005.14165), [запись лекции про GPT от сотрудников OpenAI](https://www.youtube.com/watch?v=3gb-ZkVRemQ&ab_channel=StanfordOnline)
1. Вариации BERT: [ALBERT](https://arxiv.org/abs/1909.11942), [RoBERTa](https://arxiv.org/abs/1907.11692), [DeBERTa](https://arxiv.org/abs/2006.03654), [ELECTRA](https://arxiv.org/abs/2003.10555), [XLM](https://arxiv.org/abs/1901.07291) 
1. Вариации Трансформера (seq2seq): [T5](https://arxiv.org/abs/1910.10683), [T0](https://arxiv.org/pdf/2110.08207), [BART](https://arxiv.org/abs/1910.13461), [MASS](https://arxiv.org/abs/1905.02450)
1. Трюки для обучения трансформеров: [статья](https://arxiv.org/pdf/1804.00247)
